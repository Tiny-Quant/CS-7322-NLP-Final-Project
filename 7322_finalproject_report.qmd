---
title: "Exploring the Impact of Biased Data on Fine-Tuned Language Models"
author: "Art Taychameekiatchai, Wen Fan, Cardy Pennington"
format: pdf
editor: visual
---

## Introduction

Pre-trained large language models (LLMs), such as BERT, have demonstrated remarkable success in various natural language processing tasks. However, the robustness and stability of these models when exposed to biased or adversarial data remain critical areas of research. If such models are fine-tuned with data that intentionally contradicts their original training, their predictions can shift in unpredictable or undesirable ways.

Many different strategies have been used to attack pre-trained LLMs like BERT in order to test their robustness in different scenarios. Bareno et al.'s 2010 paper categorizes types of attacks on LLMs as either "causative" or "exploratory." Causative attacks modify training data to directly impact the classifier while exploratory attacks involve intentionally crafting examples to be tested that are likely to be misclassified, essentially exploiting already-present weaknesses of the classifier.

The primary objective of this project is to evaluate the extent to which fine-tuned LLMs can be influenced by additional biased training data. To achieve this, we will fine-tune BERT for a baseline classification task, generate synthetic datasets to further train the model and assess its ability to modify classification outcomes for specific targets, and analyze whether strategic modifications to the training corpus can cause targeted changes in classification results.

### Data:

The dataset used in this project was sourced from the Kaggle repository (<https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data>). The dataset contains 44,919 news articles, each labeled as either fake news and real news. The dataset contains 23,481 fake news articles and 21,417 real news articles. Each article includes title and text, enabling thorough analysis of features that distinguish the two categories. This dataset was chosen for its comprehensiveness and reliability, making it suitable for tasks such as fine-tuning language models and evaluating classification performance.

## Methods

The data was split into training, validation, and test sets using an 80-20-20 ratio. Each article's text was augmented by concatenating the title and content fields to form a single input string. Tokenization was done using BERT tokenizer with maximum length of 128, and padding and truncation were used when needed to ensure that the sequences were the same length.

We tried three different methods to see if they changed the model's classification accuracy. One method we tried was using synthetic data to introduce bias. Key features associated with fake news were identified using TF-IDF and chi-squared tests. We considered phrases with a maximum length of five words. Next, we used those features to create 100 new synthetic news articles using GPT-2, which were labeled as real news. We thought this synthetic data could potentially challenge the model by having articles labeled as real news that included features associated with fake news. A combined dataset of the original news articles and the synthetic data was then used to retrain and finetune the BERT. We tested a range of different numbers of fake news features and numbers of synthetic articles. We considered 100, 200, 300, 400, and 500 features and 200, 300, 400, 500, 600, 700, 800, 900, and 1000 synthetic articles. The retrained model's performance was then tested on the original test set and then compared to the original model's performance. During training, AdamW optimizer and learning rate 0.00005 were used. Batch size was chosen to be 32 for training and 16 for validation and testing.

We also considered modifying the training corpus in two ways by pre-training using specific portions of the dataset. First, we pre-trained BERT on only the fake news dataset. The intention was that BERT would then be so fine tuned to recognize fake news patterns that it would potentially mistakenly classify real news as fake news. We also tested pre-training BERT using a randomly sampled word corpus. This was generated by randomly sampling words from both real and fake news articles. We intended for this pre-training to make the embeddings of fake news and real news more similar, potentially confusing the classifier.

## Results

Baseline Fine-Tuning: \~99% accuracy

Bias Introduction via Synthetic Data

Top 20 fake news features:

For all ‘i’ (number of fake news features) and ‘j’ (number of synthetic news) combinations, the test accuracy \~99% 

Baseline Fine-Tuning: \~99%

Bias Introduction via Synthetic Data: \~99% 

Pre-training on Fake News Corpus: \~99%

Randomly Sampled Word Corpus: \~99%

## Discussion

Potential Reasons for No Change in Accuracy:

1.The biased corpus is too small. It is possible that we just didn’t add enough new example to change the original BERT embedding sufficiently. Unlikely purely a numbers issue as \~ 20,000 new examples were added for the Fake New and random work sampling corpuses. 

2.The bias is not strong enough. This problem is likely true. In order to effect, the downstream model we really needed to change word embeddings a lot. We probably need more specific targeting, but these may be infeasible or impractical to actually implement.

3.The classification corpus is too large. Another potential problem that the problems lie with the classification model and has nothing to do with the pre-training. A model fine-tuned on over 40,000 labeled examples is sure to find a good binary classification boundary even if the original pre-trained embeddings are messed up. 

## Conclusion

Our findings highlight the resilience of fine-tuned BERT models against biased data. Despite various attempts to manipulate its predictions by using synthetic data or targeted pre-training, the model maintained high accuracy in distinguishing fake news from real news. This suggests that the model’s representations are distributed and robust, making them less susceptible to superficial modifications in the data.

Potential directions for future research:

Simultaneous adversarial training for biased text generation.

Reverse engineering pre-training procedures to determine if the placement of biased phrases effect how much the embeddings change. 

## References

Barreno, M., Nelson, B., Joseph, A.D. *et al.* The security of machine learning. *Mach Learn* **81**, 121–148 (2010). https://doi.org/10.1007/s10994-010-5188-5
