{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import keras\n",
    "from keras.layers import TorchModuleWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n",
      "subject\n",
      "News               9050\n",
      "politics           6841\n",
      "left-news          4459\n",
      "Government News    1570\n",
      "US_News             783\n",
      "Middle-east         778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fake_df = pd.read_csv('./data/Fake.csv')\n",
    "real_df = pd.read_csv('./data/True.csv')\n",
    "\n",
    "print(fake_df.head())\n",
    "print(fake_df.subject.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df['label'] = 0\n",
    "real_df['label'] = 1\n",
    "\n",
    "df = pd.concat([fake_df, real_df], axis=0)\n",
    "df['text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encodings['labels'] = torch.tensor(labels.tolist())\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizedDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenize_data(train['text'], train['label'], tokenizer)\n",
    "val_encodings = tokenize_data(val['text'], val['label'], tokenizer)\n",
    "test_encodings = tokenize_data(test['text'], test['label'], tokenizer)\n",
    "\n",
    "train_dataset = tokenizedDataset(train_encodings)\n",
    "val_dataset = tokenizedDataset(val_encodings)\n",
    "test_dataset = tokenizedDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=2\n",
    ")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifer(keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.model = TorchModuleWrapper(model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(model.device)\n",
    "        return self.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"classifer_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"classifer_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_8          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">109,483,778</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ torch_module_wrapper_8          │ ?                      │   \u001b[38;5;34m109,483,778\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,483,778</span> (417.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,483,778\u001b[0m (417.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,483,778</span> (417.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,483,778\u001b[0m (417.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifer = Classifer(model)\n",
    "classifer.summary()\n",
    "classifer.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0909, -0.2192],\n",
      "        [-0.1132, -0.0588],\n",
      "        [ 0.1423, -0.3546],\n",
      "        [-0.1873,  0.0442],\n",
      "        [-0.1585, -0.3127],\n",
      "        [-0.2364, -0.0462],\n",
      "        [-0.0224, -0.2365],\n",
      "        [-0.0529, -0.0844],\n",
      "        [-0.0209, -0.3082],\n",
      "        [ 0.1284, -0.0803],\n",
      "        [-0.0448,  0.1449],\n",
      "        [-0.0776, -0.0340],\n",
      "        [ 0.1214, -0.0387],\n",
      "        [ 0.1586,  0.0493],\n",
      "        [-0.1997, -0.1001],\n",
      "        [ 0.0618,  0.0485],\n",
      "        [ 0.2774, -0.1057],\n",
      "        [ 0.0538,  0.0725],\n",
      "        [-0.1297, -0.2052],\n",
      "        [-0.1642, -0.2206],\n",
      "        [ 0.1444, -0.2110],\n",
      "        [ 0.0444,  0.0609],\n",
      "        [ 0.1077, -0.2176],\n",
      "        [-0.2211, -0.0356],\n",
      "        [-0.0419,  0.0767],\n",
      "        [-0.1746,  0.0142],\n",
      "        [ 0.1728, -0.2108],\n",
      "        [ 0.0185, -0.1811],\n",
      "        [-0.0279, -0.0167],\n",
      "        [ 0.0717, -0.0328],\n",
      "        [-0.0215, -0.0668],\n",
      "        [ 0.1143, -0.1177]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    out = classifer(batch)\n",
    "    print(out)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
